{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.spatial import ConvexHull\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from itertools import combinations\n",
    "\n",
    "def get_commont_vertex(edge_pair):\n",
    "    a = edge_pair[:, 0] == edge_pair[:, 1]\n",
    "    b = edge_pair[:, 0] == torch.flip(edge_pair[:, 1], dims=[1])\n",
    "\n",
    "    return edge_pair[:, 0][a + b]\n",
    "\n",
    "class Non(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Non, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "def adjacency_matrix(vertices, faces):\n",
    "    B, N, D = vertices.shape\n",
    "\n",
    "    halfedges = torch.tensor(list(combinations(range(D), 2)))\n",
    "    edges = torch.cat([halfedges, torch.flip(halfedges,dims=[1])], dim=0)\n",
    "\n",
    "\n",
    "    A = torch.zeros(1, N, N, device=faces.device)\n",
    "\n",
    "    all_edges = faces[:, :, edges].long()\n",
    "    all_edges = all_edges.view(1, -1, 2)\n",
    "    A[0, all_edges[0, :, 0], all_edges[0, :, 1]] = 1 \n",
    "    D = torch.diag(1 / torch.squeeze(torch.sum(A, dim=1)))[None]\n",
    "\n",
    "    A = A.repeat(B, 1, 1)\n",
    "    D = D.repeat(B, 1, 1)\n",
    "\n",
    "    return A, D \n",
    "\n",
    "\n",
    "def adaptive_unpool(vertices, faces_prev, sphere_vertices, latent_features, N_prev):\n",
    "    print(\"vertices\", vertices.shape)\n",
    "    vertices_primary = vertices[0,:N_prev, :]\n",
    "    print(\"vertices_primary\", vertices_primary.shape)\n",
    "    vertices_secondary = vertices[0,N_prev:, :]\n",
    "    faces_primary = faces_prev[0]\n",
    "    print(\"vertices_secondary\", vertices_secondary.shape)\n",
    "    sphere_vertices_primary = sphere_vertices[0,:N_prev]\n",
    "    sphere_vertices_secondary = sphere_vertices[0,N_prev:]\n",
    "\n",
    "    if latent_features is not None:\n",
    "        latent_features_primary = latent_features[0,:N_prev]\n",
    "        latent_features_secondary = latent_features[0,N_prev:]\n",
    "\n",
    "    face_count, _ = faces_primary.shape\n",
    "    vertices_count = len(vertices_primary)\n",
    "    edge_combinations_3 = torch.tensor(list(combinations(range(3), 2)), device = vertices.device)\n",
    "    edges = faces_primary[:, edge_combinations_3]\n",
    "    unique_edges = edges.view(-1, 2)\n",
    "    unique_edges, _ = torch.sort(unique_edges, dim=1)\n",
    "    unique_edges, unique_edge_indices = torch.unique(unique_edges, return_inverse=True, dim=0)\n",
    "    print(\"unique_edges\", unique_edges.shape)\n",
    "    face_edges_primary = vertices_primary[unique_edges]\n",
    "    print(\"face_edges\", face_edges_primary.shape)\n",
    "    print(type(face_edges_primary))\n",
    "\n",
    "    a = face_edges_primary[:,0]\n",
    "    b = face_edges_primary[:,1]\n",
    "    v = vertices_secondary\n",
    "    print(\"a\", a.shape)\n",
    "    print(\"b\", b.shape)\n",
    "    print(\"v\", v.shape) \n",
    "\n",
    "    va = v - a\n",
    "    vb = v - b\n",
    "    ba = b - a\n",
    "\n",
    "    cond1 = (va * ba).sum(1)\n",
    "    norm1 = torch.norm(va, dim=1)\n",
    "\n",
    "    cond2 = (vb * ba).sum(1)\n",
    "    norm2 = torch.norm(vb, dim=1)\n",
    "\n",
    "    dist = torch.norm(torch.cross(va, ba), dim=1)/torch.norm(ba, dim=1)\n",
    "    dist[cond1 < 0] = norm1[cond1 < 0]\n",
    "    dist[cond2 < 0] = norm2[cond2 < 0]\n",
    "\n",
    "    sorted_, _ = torch.sort(dist)\n",
    "    threshold = sorted_[int(0.3*len(sorted_))] \n",
    "\n",
    "    vertices_needed = vertices_secondary[dist > threshold]\n",
    "    \n",
    "    sphere_vertices_needed = sphere_vertices_secondary[dist > threshold] \n",
    "    if latent_features is not None:\n",
    "        latent_features_needed = latent_features_secondary[dist > threshold]\n",
    "\n",
    "    vertices = torch.cat([vertices_primary,vertices_needed],dim=0)[None]\n",
    "    if latent_features is not None:\n",
    "        latent_features = torch.cat([latent_features_primary,latent_features_needed],dim=0)[None]\n",
    "\n",
    "    sphere_vertices = torch.cat([sphere_vertices_primary,sphere_vertices_needed],dim=0) \n",
    "    sphere_vertices = sphere_vertices/torch.sqrt(torch.sum(sphere_vertices**2,dim=1)[:,None])\n",
    "    hull = ConvexHull(sphere_vertices.data.cpu().numpy())  \n",
    "    faces = torch.from_numpy(hull.simplices).long().cuda()[None] \n",
    "\n",
    "    sphere_vertices = sphere_vertices[None]  \n",
    "\n",
    "    return vertices, faces, latent_features, sphere_vertices\n",
    "\n",
    "class GraphConv(nn.Module):\n",
    "    __constants__ = ['bias', 'in_features', 'out_features']\n",
    "\n",
    "    def __init__(self, in_features, out_features, batch_norm=False):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.neighbours_fc = nn.Linear(in_features, out_features)\n",
    "\n",
    "        self.bc = nn.BatchNorm1d(out_features) if batch_norm else Non()\n",
    "\n",
    "    def forward(self, input, A, Dinv, vertices, faces):\n",
    "\n",
    "        # coeff = torch.bmm(torch.bmm(Dsqrtinv, A), Dsqrtinv)\n",
    "        coeff = torch.bmm(Dinv, A) # row normalization, zmienione z bmm na mm ze względu na batch 1\n",
    "        #coeff ma na celu wyrownanie wpływu wierzchołków o większej liczbie sąsiadów na deformację, dlatego to nie ejst samo A, tylko pomnożone przez Dinv\n",
    "        y = self.fc(input) #to jest zwykła transformacja liniowa\n",
    "        y_neightbours = torch.bmm(coeff, input)#zmienione bmm na matmul, ze względu na batch 1.\n",
    "        #linijka wyżej mnoży współczynnik z cechami. Współczynnik jest reprezentacją connectivity w meshu deformowanym, więc to jest przerzucenie cech na siatkę\n",
    "        y_neightbours = self.neighbours_fc(y_neightbours)\n",
    " \n",
    " \n",
    "        # y_neightbours = self.bc(y_neightbours.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        y = y + y_neightbours\n",
    "        # y = self.bc(y.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        return y\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}'.format(\n",
    "            self.in_features, self.out_features is not None\n",
    "        )\n",
    "    \n",
    "\n",
    "class Feature2VertexLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, hidden_layer_count, batch_norm=False):\n",
    "        super(Feature2VertexLayer, self).__init__()\n",
    "        self.gconv = []\n",
    "        for i in range(hidden_layer_count, 1, -1):\n",
    "            self.gconv += [GraphConv(i * in_features // hidden_layer_count, (i-1) * in_features // hidden_layer_count, batch_norm)]\n",
    "        self.gconv_layer = nn.Sequential(*self.gconv)\n",
    "        self.gconv_last = GraphConv(in_features // hidden_layer_count, 3, batch_norm)\n",
    "\n",
    "    def forward(self, features, adjacency_matrix, degree_matrix, vertices, faces):\n",
    "        for gconv_hidden in self.gconv:\n",
    "            features = F.relu(gconv_hidden(features, adjacency_matrix, degree_matrix,vertices,faces))\n",
    "        return self.gconv_last(features, adjacency_matrix, degree_matrix,vertices,faces)\n",
    "\n",
    "class Features2Features(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, hidden_layer_count=2, graph_conv=GraphConv):\n",
    "        super(Features2Features, self).__init__()\n",
    "\n",
    "        self.gconv_first = graph_conv(in_features, out_features)\n",
    "        gconv_hidden = []\n",
    "        for i in range(hidden_layer_count):\n",
    "            gconv_hidden += [graph_conv(out_features, out_features)]\n",
    "        self.gconv_hidden = nn.Sequential(*gconv_hidden)\n",
    "        self.gconv_last = graph_conv(out_features, out_features)\n",
    "\n",
    "    def forward(self, features, adjacency_matrix, degree_matrix, vertices, faces):\n",
    "        features = F.relu(self.gconv_first(features, adjacency_matrix, degree_matrix, vertices,faces))\n",
    "        for gconv_hidden in self.gconv_hidden:\n",
    "            features = F.relu(gconv_hidden(features, adjacency_matrix, degree_matrix, vertices,faces))\n",
    "        return self.gconv_last(features, adjacency_matrix, degree_matrix, vertices, faces)\n",
    "\n",
    "def uniform_unpool(vertices_, faces_, identical_face_batch=True):\n",
    "    if vertices_ is None:\n",
    "        return None, None\n",
    "    batch_size , _, _ = vertices_.shape\n",
    "    new_faces_all = []\n",
    "    new_vertices_all = []\n",
    "\n",
    "    for vertices, faces in zip(vertices_, faces_):\n",
    "        face_count, _ = faces.shape\n",
    "        vertices_count = len(vertices)\n",
    "        edge_combinations_3 = torch.tensor(list(combinations(range(3), 2)))\n",
    "        edges = faces[:, edge_combinations_3]\n",
    "        unique_edges = edges.view(-1, 2)\n",
    "        unique_edges, _ = torch.sort(unique_edges, dim=1)\n",
    "        unique_edges, unique_edge_indices = torch.unique(unique_edges, return_inverse=True, dim=0)\n",
    "        face_edges = vertices[unique_edges]\n",
    "\n",
    "        ''' Computer new vertices '''\n",
    "        new_vertices = torch.mean(face_edges, dim=1)\n",
    "        new_vertices = torch.cat([vertices, new_vertices], dim=0)  # <----------------------- new vertices + old vertices\n",
    "        new_vertices_all += [new_vertices[None]]\n",
    "\n",
    "        ''' Compute new faces '''\n",
    "        corner_faces = []\n",
    "        middle_face = []\n",
    "        for j, combination in enumerate(edge_combinations_3):\n",
    "            edge_pair = edges[:, combination]\n",
    "            common_vertex = get_commont_vertex(edge_pair)\n",
    "\n",
    "            new_vertex_1 = unique_edge_indices[torch.arange(0, 3 * face_count, 3) + combination[0]] + vertices_count\n",
    "            new_vertex_2 = unique_edge_indices[torch.arange(0, 3 * face_count, 3) + combination[1]] + vertices_count\n",
    "\n",
    "            middle_face += [new_vertex_1[:, None], new_vertex_2[:, None]]\n",
    "            corner_faces += [torch.cat([common_vertex[:, None], new_vertex_1[:, None], new_vertex_2[:, None]], dim=1)]\n",
    "\n",
    "        corner_faces = torch.cat(corner_faces, dim=0)\n",
    "        middle_face = torch.cat(middle_face, dim=1)\n",
    "        middle_face = torch.unique(middle_face, dim=1)\n",
    "        new_faces_all += [torch.cat([corner_faces, middle_face], dim=0)[None]]  # new faces-3\n",
    "\n",
    "        if identical_face_batch:\n",
    "            new_vertices_all = new_vertices_all[0].repeat(batch_size, 1, 1)\n",
    "            new_faces_all = new_faces_all[0].repeat(batch_size, 1, 1)\n",
    "            break\n",
    "\n",
    "    return new_vertices_all, new_faces_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"latent_features_count\": [1024, 32],\n",
    "    \"graph_conv_layer_count\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeshDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(MeshDecoder, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.graph_conv_net = Features2Features(config[\"latent_features_count\"][0], config[\"latent_features_count\"][1], hidden_layer_count=config[\"graph_conv_layer_count\"])\n",
    "        self.feature_to_vertex = Feature2VertexLayer(config[\"latent_features_count\"][1], 3)\n",
    "\n",
    "    def forward(self, vertices, faces, latent_features):\n",
    "\n",
    "        #A, D = adjacency_matrix(vertices, faces) #jeśli dobrze rozumiem działanie tej funkcji?\n",
    "\n",
    "        _, N_prev, _ = vertices.shape \n",
    "        #to jest grafowa CNN do przekształcenia cech z enkodera do cech które przyjmie F2V\n",
    "        #latent_features = self.graph_conv_net(latent_features, A, D, vertices, faces)\n",
    "        vertices, faces_ = uniform_unpool(vertices, faces)  \n",
    "        latent_features, _ = uniform_unpool(latent_features, faces)\n",
    "        faces = faces_\n",
    "        print(vertices.shape)\n",
    "        print(vertices.shape)\n",
    "        A, D = adjacency_matrix(vertices, faces)\n",
    "        updated_latent_features = self.graph_conv_net(latent_features, A, D, vertices, faces)\n",
    "\n",
    "        #być może latent_features = torch.cat()\n",
    "\n",
    "        #f2v - obliczenie wektorów deformacji z cech, czyli przeniesienie na przestrzeń wierzchołków? współrzędnych?\n",
    "        deformation_vectors = self.feature_to_vertex(updated_latent_features, A, D, vertices, faces)\n",
    "\n",
    "        #deformacja siatki - przesunięcie o wektor wierzchołków. Faces się nie zmieniają, bo one wskazują wierzchołki, nie mają współrzędnych\n",
    "        deformed_vertices = vertices + deformation_vectors\n",
    "\n",
    "        #unpooling - upsampling, może być uniform, ale adaptive jest lepszy, bo nie dodaje niepotrzebnych wierzchołków na płaskich przestrzeniach.\n",
    "        upsampled_vertices, upsampled_faces = adaptive_unpool(vertices, faces, deformed_vertices, latent_features, N_prev)\n",
    "\n",
    "        return upsampled_vertices, upsampled_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 4456416 into shape (3882114,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mesh\n\u001b[1;32m      4\u001b[0m SPHERE_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSphere.stl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m vertices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0a0f3b60/vertices.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m faces \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0a0f3b60/faces.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m latent_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1250\u001b[39m, \u001b[38;5;241m1024\u001b[39m)\n",
      "File \u001b[0;32m~/Python/Medshapenet/venv/lib/python3.11/site-packages/numpy/lib/npyio.py:456\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[1;32m    454\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m~/Python/Medshapenet/venv/lib/python3.11/site-packages/numpy/lib/format.py:839\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    837\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 839\u001b[0m         \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m \u001b[38;5;241m=\u001b[39m shape\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 4456416 into shape (3882114,3)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stl import mesh\n",
    "\n",
    "SPHERE_PATH = \"Sphere.stl\"\n",
    "\n",
    "vertices = np.load(\"0a0f3b60/vertices.npy\")\n",
    "faces = np.load(\"0a0f3b60/faces.npy\")\n",
    "\n",
    "latent_features = torch.rand(1, 1250, 1024)\n",
    "\n",
    "loaded_mesh = mesh.Mesh.from_file(SPHERE_PATH)\n",
    "vertex_dict = {} #for storing vertices and their indices, helps to avoid duplicating the same vertices\n",
    "sphere_vertices = np.empty((0, 3), dtype=float) #array of vertices - shape (n, 3), stores x, y, z coordinates of each vertex\n",
    "sphere_faces = [] #array of faces - shape (n, 3), stores indices of vertices in each face\n",
    "\n",
    "for vectors in loaded_mesh.vectors:\n",
    "        for vertex in vectors:\n",
    "            vertex_tuple = tuple(vertex)\n",
    "\n",
    "            if vertex_tuple in vertex_dict:\n",
    "                index = vertex_dict[vertex_tuple]\n",
    "            else:\n",
    "                index = len(sphere_vertices)\n",
    "                sphere_vertices = np.append(sphere_vertices, [vertex], axis=0)\n",
    "                vertex_dict[vertex_tuple] = index\n",
    "\n",
    "        sphere_faces.append([vertex_dict[tuple(vertex)] for vertex in vectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sphere_vertices, sphere_faces = torch.tensor(sphere_vertices), torch.tensor(sphere_faces)\n",
    "vertices, faces = torch.tensor(vertices), torch.tensor(faces)\n",
    "\n",
    "sphere_faces = sphere_faces.unsqueeze(0)\n",
    "sphere_vertices = sphere_vertices.unsqueeze(0)\n",
    "\n",
    "decoder = MeshDecoder(config)\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    deformed_vertices, deformed_faces = decoder(sphere_vertices, sphere_faces, latent_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
